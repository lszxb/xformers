


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
<!--<![endif]-->

<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>xformers.ops.memory_efficient_attention | xFormers 0.0.15.dev documentation</title>
  
  <script src="../../../_static/js/ga.js"></script>
  <script src="../../../_static/js/redirect.js"></script>
  
  <link rel="shortcut icon" href="../../../_static/images/favicon.png" />
  
  
  <link rel="canonical" href="https://facebookresearch.github.io/xformers_modules/xformers/ops/memory_efficient_attention.html" />
  
  <meta property="og:title" content="xformers.ops.memory_efficient_attention | xFormers 0.0.15.dev documentation">
  <meta name="description" content="API docs for xFormers. xFormers is a PyTorch extension library for composable and optimized Transformer blocks.">
  <meta property="og:description" content="API docs for xFormers. xFormers is a PyTorch extension library for composable and optimized Transformer blocks.">
  <!--<meta property="og:image" content="https://mmf.sh/img/logo.png">-->
  <!--<meta property="twitter:image" content="https://mmf.sh/img/logo.png">-->
  <meta name="twitter:image:alt" content="Image for xFormers">
  <meta name="twitter:card" content="summary_large_image">
  

  
  
  

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../_static/css/customize.css" type="text/css" />
  <link rel="index" title="Index" href="../../../genindex.html" />
  <link rel="search" title="Search" href="../../../search.html" /> 

  
  <script src="../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://github.com/facebookresearch/xformers"><img
          src="../../../_static/logo.png" style="padding-right: 90px;" width="280px" height="53px"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/facebookresearch/xformers"> xFormers Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   

  

  <div class="table-of-contents-link-wrapper">
    <span>Table of Contents</span>
    <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
  </div>

  <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
    <div class="pytorch-side-scroll">
      <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <div class="pytorch-left-menu-search">
          

          
          
          
          

          


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        
        
        
        
        
        <p class="caption" role="heading"><span class="caption-text">Index</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../what_is_xformers.html">What is xFormers?</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Components Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../components/index.html">API Reference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build models and blocks programatically</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../factory/index.html">Factory</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials and examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/index.html">Tutorials</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Some custom parts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../custom_parts/index.html">Custom parts reference</a></li>
</ul>

        
        
      </div>
    </div>
  </nav>

  <div class="pytorch-container">
    <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
      <div class="pytorch-breadcrumbs-wrapper">
        















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
      <li>xformers.ops.memory_efficient_attention</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
      </div>

      <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
        Shortcuts
      </div>
    </div>

    <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
      <div class="pytorch-content-left">

        
        
          <div class="rst-content">
            
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
              <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
                
  <h1>Source code for xformers.ops.memory_efficient_attention</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># This source code is licensed under the BSD license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>


<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span><span class="p">,</span> <span class="n">replace</span>
<span class="kn">from</span> <span class="nn">types</span> <span class="kn">import</span> <span class="n">SimpleNamespace</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Mapping</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Set</span><span class="p">,</span> <span class="n">Type</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">from</span> <span class="nn">.common</span> <span class="kn">import</span> <span class="n">get_xformers_operator</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">..</span> <span class="kn">import</span> <span class="n">_C_flashattention</span>  <span class="c1"># type: ignore[attr-defined]</span>

    <span class="n">has_flashattention</span> <span class="o">=</span> <span class="kc">True</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="n">has_flashattention</span> <span class="o">=</span> <span class="kc">False</span>


<div class="viewcode-block" id="AttentionMask"><a class="viewcode-back" href="../../../components/ops.html#xformers.ops.AttentionMask">[docs]</a><span class="k">class</span> <span class="nc">AttentionMask</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Base class for custom masks that can be applied \</span>
<span class="sd">        in :attr:`xformers.ops.memory_efficient_attention`.</span>

<span class="sd">    When using an :attr:`xformers.ops.AttentionMask`</span>
<span class="sd">    instead of a :attr:`torch.Tensor`, the mask matrix does</span>
<span class="sd">    not need to be materialized, and can be</span>
<span class="sd">    hardcoded into some kernels for better performance.</span>

<span class="sd">    See also :attr:`xformers.ops.LowerTriangularMask`</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="AttentionMask.to_tensor"><a class="viewcode-back" href="../../../components/ops.html#xformers.ops.AttentionMask.to_tensor">[docs]</a>    <span class="k">def</span> <span class="nf">to_tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Materializes the mask tensor</span>

<span class="sd">        Returns:</span>
<span class="sd">            torch.Tensor</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span></div></div>


<div class="viewcode-block" id="LowerTriangularMask"><a class="viewcode-back" href="../../../components/ops.html#xformers.ops.LowerTriangularMask">[docs]</a><span class="k">class</span> <span class="nc">LowerTriangularMask</span><span class="p">(</span><span class="n">AttentionMask</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A lower triangular mask that can be used for causal attention&quot;&quot;&quot;</span>

<div class="viewcode-block" id="LowerTriangularMask.__init__"><a class="viewcode-back" href="../../../components/ops.html#xformers.ops.LowerTriangularMask.__init__">[docs]</a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">tensor_args</span><span class="p">,</span> <span class="o">**</span><span class="n">tensor_kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Creates a Lower triangular mask.</span>
<span class="sd">        It is not requires to specify any parameter, as they are only \</span>
<span class="sd">            used when calling :attr:`LowerTriangularMask.to_tensor`</span>

<span class="sd">        The mask will not be materialized by default, and hence does not use \</span>
<span class="sd">            any additional memory, but acts as an option for the MHA kernel.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tensor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_kwargs</span> <span class="o">=</span> <span class="n">tensor_kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_args</span> <span class="o">=</span> <span class="n">tensor_args</span></div>

<div class="viewcode-block" id="LowerTriangularMask.to_tensor"><a class="viewcode-back" href="../../../components/ops.html#xformers.ops.LowerTriangularMask.to_tensor">[docs]</a>    <span class="k">def</span> <span class="nf">to_tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Materializes the mask tensor</span>

<span class="sd">        Returns:</span>
<span class="sd">            torch.Tensor</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensor</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Work around for &quot;triu_tril_cuda_template&quot; not implemented for &#39;BFloat16&#39;</span>
            <span class="n">dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;dtype&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
            <span class="n">create_as</span> <span class="o">=</span> <span class="n">dtype</span> <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
                <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">_tensor_args</span><span class="p">,</span>
                <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">_tensor_kwargs</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">create_as</span><span class="p">,</span>
                <span class="n">fill_value</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">),</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tensor</span><span class="p">,</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensor</span></div></div>


<div class="viewcode-block" id="AttentionOpBase"><a class="viewcode-back" href="../../../components/ops.html#xformers.ops.AttentionOpBase">[docs]</a><span class="k">class</span> <span class="nc">AttentionOpBase</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Base class for any attention operator in xFormers</span>

<span class="sd">    See:</span>

<span class="sd">    - :attr:`xformers.ops.MemoryEfficientAttentionOp`</span>

<span class="sd">    - :attr:`xformers.ops.MemoryEfficientAttentionCutlassOp`</span>

<span class="sd">    - :attr:`xformers.ops.MemoryEfficientAttentionFlashAttentionOp`</span>

<span class="sd">    - :attr:`xformers.ops.MemoryEfficientAttentionCutlassFwdFlashBwOp`</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">FORWARD_OPERATOR</span><span class="p">:</span> <span class="n">Any</span>
    <span class="n">FORWARD_ERROR_ATOL</span><span class="p">:</span> <span class="n">Mapping</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">:</span> <span class="mf">3e-4</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">half</span><span class="p">:</span> <span class="mf">4e-3</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">:</span> <span class="mf">2e-2</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="n">FORWARD_ERROR_RTOL</span><span class="p">:</span> <span class="n">Mapping</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">:</span> <span class="mf">2e-5</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">half</span><span class="p">:</span> <span class="mf">4e-4</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">:</span> <span class="mf">5e-3</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="n">SUPPORTED_DEVICES</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
    <span class="n">SUPPORTED_DTYPES</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span>
    <span class="n">SUPPORTED_MAX_K</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">SUPPORTED_ATTN_BIAS_TYPES</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="nb">type</span><span class="p">(</span><span class="kc">None</span><span class="p">)}</span>
    <span class="n">SUPPORTS_DROPOUT</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="n">SUPPORTS_CUSTOM_SCALE</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">SUPPORTS_DIFFERENT_VALUE_EMBED</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">NAME</span><span class="p">:</span> <span class="nb">str</span>

    <span class="n">_TEST_BATCH_SIZES</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">300</span><span class="p">]</span>
    <span class="n">_TEST_K</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">]</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">info</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">cls</span><span class="o">.</span><span class="n">FORWARD_OPERATOR</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;no_such_operator&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="s2">&quot;not built&quot;</span>
        <span class="k">return</span> <span class="s2">&quot;available&quot;</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">forward_no_grad</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">query</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">key</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attn_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">AttentionMask</span><span class="p">]],</span>
        <span class="n">p</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">ctx</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attn_bias</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">scale</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">ctx</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">supports</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">d</span><span class="p">:</span> <span class="s2">&quot;AttentionOpDispatch&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="n">device_type</span> <span class="o">=</span> <span class="n">d</span><span class="o">.</span><span class="n">device</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="n">d</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span>
        <span class="k">if</span> <span class="n">device_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">SUPPORTED_DEVICES</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">d</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">SUPPORTED_DTYPES</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">cls</span><span class="o">.</span><span class="n">SUPPORTS_DIFFERENT_VALUE_EMBED</span> <span class="ow">and</span> <span class="n">d</span><span class="o">.</span><span class="n">k</span> <span class="o">!=</span> <span class="n">d</span><span class="o">.</span><span class="n">kv</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="nb">max</span><span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">k</span><span class="p">,</span> <span class="n">d</span><span class="o">.</span><span class="n">kv</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">cls</span><span class="o">.</span><span class="n">SUPPORTED_MAX_K</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">d</span><span class="o">.</span><span class="n">attn_bias_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">SUPPORTED_ATTN_BIAS_TYPES</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">d</span><span class="o">.</span><span class="n">has_dropout</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">cls</span><span class="o">.</span><span class="n">SUPPORTS_DROPOUT</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">d</span><span class="o">.</span><span class="n">has_custom_scale</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">cls</span><span class="o">.</span><span class="n">SUPPORTS_CUSTOM_SCALE</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="c1"># bfloat16 is only supported on A100+</span>
        <span class="c1"># ... although the kernels can still run and give the</span>
        <span class="c1"># correct result</span>
        <span class="k">if</span> <span class="n">d</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">is</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span> <span class="ow">and</span> <span class="p">(</span>
            <span class="ow">not</span> <span class="n">device_type</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
            <span class="ow">or</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_capability</span><span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">device</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">8</span>
        <span class="p">):</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="k">return</span> <span class="kc">True</span></div>


<span class="n">AttentionOp</span> <span class="o">=</span> <span class="n">Type</span><span class="p">[</span><span class="n">AttentionOpBase</span><span class="p">]</span>


<div class="viewcode-block" id="MemoryEfficientAttentionOp"><a class="viewcode-back" href="../../../components/ops.html#xformers.ops.MemoryEfficientAttentionOp">[docs]</a><span class="k">class</span> <span class="nc">MemoryEfficientAttentionOp</span><span class="p">(</span><span class="n">AttentionOpBase</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;An operator optimized for very small values of K (``K &lt;= 32``) \</span>
<span class="sd">        and f32 pre-Ampere as it does not use TensorCores.</span>
<span class="sd">    Only supports contiguous inputs in BMK format, so an extra reshape \</span>
<span class="sd">        or contiguous call might be done.</span>

<span class="sd">    :Deprecated:</span>

<span class="sd">        This operator is deprecated and should not be used in new code</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">FORWARD_OPERATOR</span> <span class="o">=</span> <span class="n">get_xformers_operator</span><span class="p">(</span><span class="s2">&quot;efficient_attention&quot;</span><span class="p">)</span>
    <span class="n">SUPPORTED_DEVICES</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="s2">&quot;cpu&quot;</span><span class="p">}</span>
    <span class="n">SUPPORTED_DTYPES</span> <span class="o">=</span> <span class="p">{</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">}</span>
    <span class="n">SUPPORTED_MAX_K</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">32</span>
    <span class="n">SUPPORTED_ATTN_BIAS_TYPES</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="nb">type</span><span class="p">(</span><span class="kc">None</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">}</span>
    <span class="n">SUPPORTS_DROPOUT</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">SUPPORTS_CUSTOM_SCALE</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">NAME</span> <span class="o">=</span> <span class="s2">&quot;small_k&quot;</span>

    <span class="c1"># as this kernel is a bit slow, this should make tests run faster</span>
    <span class="n">_TEST_BATCH_SIZES</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
    <span class="n">_TEST_K</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">]</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">supports</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">d</span><span class="p">:</span> <span class="s2">&quot;AttentionOpDispatch&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">super</span><span class="p">(</span><span class="n">MemoryEfficientAttentionOp</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">supports</span><span class="p">(</span><span class="n">d</span><span class="p">):</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="n">buffer_size</span> <span class="o">=</span> <span class="mi">8</span>
        <span class="k">for</span> <span class="n">pack</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">]:</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">k</span> <span class="o">%</span> <span class="n">pack</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">k</span> <span class="o">//</span> <span class="n">pack</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">buffer_size</span><span class="p">:</span>
                <span class="k">return</span> <span class="kc">True</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">bmhk2bmk_contiguous</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="n">tensor</span><span class="o">.</span><span class="n">permute</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
            <span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
            <span class="o">.</span><span class="n">view</span><span class="p">([</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]])</span>
            <span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">bmk2bmhk</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">tensor</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]]</span>
        <span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">forward_no_grad</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">query</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">key</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attn_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">AttentionMask</span><span class="p">]],</span>
        <span class="n">p</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">scale</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Unsupport custom scale&quot;</span><span class="p">)</span>
        <span class="n">num_heads</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">query</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">bmhk2bmk_contiguous</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">bmhk2bmk_contiguous</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">bmhk2bmk_contiguous</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_forward_no_grad_bmk</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attn_bias</span><span class="o">=</span><span class="n">attn_bias</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">bmk2bmhk</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">ctx</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attn_bias</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">scale</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">scale</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Unsupport custom scale&quot;</span><span class="p">)</span>
        <span class="n">num_heads</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">query</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">bmhk2bmk_contiguous</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">bmhk2bmk_contiguous</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">bmhk2bmk_contiguous</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_forward_bmk</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attn_bias</span><span class="o">=</span><span class="n">attn_bias</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">bmk2bmhk</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">ctx</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
        <span class="n">num_heads</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">bmhk2bmk_contiguous</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
        <span class="n">gq</span><span class="p">,</span> <span class="n">gk</span><span class="p">,</span> <span class="n">gv</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_backward_bmk</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>
        <span class="n">gq</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">bmk2bmhk</span><span class="p">(</span><span class="n">gq</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
        <span class="n">gk</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">bmk2bmhk</span><span class="p">(</span><span class="n">gk</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
        <span class="n">gv</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">bmk2bmhk</span><span class="p">(</span><span class="n">gv</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">gq</span><span class="p">,</span> <span class="n">gk</span><span class="p">,</span> <span class="n">gv</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_forward_no_grad_bmk</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">query</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">key</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attn_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">AttentionMask</span><span class="p">]],</span>
        <span class="n">p</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">FORWARD_OPERATOR</span><span class="p">(</span>
            <span class="n">query</span><span class="o">=</span><span class="n">query</span><span class="p">,</span>
            <span class="n">key</span><span class="o">=</span><span class="n">key</span><span class="p">,</span>
            <span class="n">value</span><span class="o">=</span><span class="n">value</span><span class="p">,</span>
            <span class="n">compute_logsumexp</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">attn_bias</span><span class="o">=</span><span class="n">attn_bias</span><span class="p">,</span>
            <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span>
        <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_forward_bmk</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">ctx</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attn_bias</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
        <span class="n">out</span><span class="p">,</span> <span class="n">lse</span><span class="p">,</span> <span class="n">rng_seed</span><span class="p">,</span> <span class="n">rng_offset</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">FORWARD_OPERATOR</span><span class="p">(</span>
            <span class="n">query</span><span class="o">=</span><span class="n">query</span><span class="p">,</span>
            <span class="n">key</span><span class="o">=</span><span class="n">key</span><span class="p">,</span>
            <span class="n">value</span><span class="o">=</span><span class="n">value</span><span class="p">,</span>
            <span class="n">compute_logsumexp</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">attn_bias</span><span class="o">=</span><span class="n">attn_bias</span><span class="p">,</span>
            <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">lse</span><span class="p">,</span> <span class="n">attn_bias</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">p</span> <span class="o">=</span> <span class="n">p</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">rng_seed</span> <span class="o">=</span> <span class="n">rng_seed</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">rng_offset</span> <span class="o">=</span> <span class="n">rng_offset</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_backward_bmk</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
        <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">lse</span><span class="p">,</span> <span class="n">attn_bias</span><span class="p">,</span> <span class="n">out</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">p</span>
        <span class="n">rng_seed</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">rng_seed</span>
        <span class="n">rng_offset</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">rng_offset</span>
        <span class="n">grad_q</span><span class="p">,</span> <span class="n">grad_k</span><span class="p">,</span> <span class="n">grad_v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">xformers</span><span class="o">.</span><span class="n">efficient_attention_backward</span><span class="p">(</span>
            <span class="n">grad</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">lse</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">attn_bias</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">rng_seed</span><span class="p">,</span> <span class="n">rng_offset</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">grad_q</span><span class="p">,</span> <span class="n">grad_k</span><span class="p">,</span> <span class="n">grad_v</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="MemoryEfficientAttentionCutlassOp"><a class="viewcode-back" href="../../../components/ops.html#xformers.ops.MemoryEfficientAttentionCutlassOp">[docs]</a><span class="k">class</span> <span class="nc">MemoryEfficientAttentionCutlassOp</span><span class="p">(</span><span class="n">AttentionOpBase</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;xFormers&#39; MHA kernel based on CUTLASS.</span>
<span class="sd">    Supports a large number of settings (including without TensorCores, f32 ...)</span>
<span class="sd">    and GPUs as old as P100 (Sm60)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">FORWARD_OPERATOR</span> <span class="o">=</span> <span class="n">get_xformers_operator</span><span class="p">(</span><span class="s2">&quot;efficient_attention_forward_cutlass&quot;</span><span class="p">)</span>
    <span class="n">SUPPORTED_DEVICES</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;cuda&quot;</span><span class="p">}</span>
    <span class="n">SUPPORTED_DTYPES</span> <span class="o">=</span> <span class="p">{</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">half</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">}</span>
    <span class="n">SUPPORTED_MAX_K</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">inf</span>
    <span class="n">SUPPORTED_ATTN_BIAS_TYPES</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="nb">type</span><span class="p">(</span><span class="kc">None</span><span class="p">),</span> <span class="n">LowerTriangularMask</span><span class="p">}</span>
    <span class="n">SUPPORTS_DROPOUT</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">SUPPORTS_CUSTOM_SCALE</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">SUPPORTS_DIFFERENT_VALUE_EMBED</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">NAME</span> <span class="o">=</span> <span class="s2">&quot;cutlass&quot;</span>

    <span class="n">_TEST_K</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span>
        <span class="mi">32</span><span class="p">,</span>  <span class="c1"># 64x64 kernel</span>
        <span class="mi">128</span><span class="p">,</span>  <span class="c1"># 64x128 kernel</span>
        <span class="mi">256</span><span class="p">,</span>  <span class="c1"># 64x128 with accumulation in gmem</span>
    <span class="p">]</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">forward_no_grad</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">query</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">key</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attn_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">AttentionMask</span><span class="p">]],</span>
        <span class="n">p</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">attn_bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">attn_bias</span><span class="p">,</span> <span class="n">LowerTriangularMask</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Unsupported attn_bias type&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">FORWARD_OPERATOR</span><span class="p">(</span>
            <span class="n">query</span><span class="o">=</span><span class="n">query</span><span class="p">,</span>
            <span class="n">key</span><span class="o">=</span><span class="n">key</span><span class="p">,</span>
            <span class="n">value</span><span class="o">=</span><span class="n">value</span><span class="p">,</span>
            <span class="n">cu_seqlens_q</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">cu_seqlens_k</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">max_seqlen_q</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">compute_logsumexp</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">causal</span><span class="o">=</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">attn_bias</span><span class="p">,</span> <span class="n">LowerTriangularMask</span><span class="p">),</span>
            <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span>
        <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">ctx</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attn_bias</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">scale</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">attn_bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">attn_bias</span><span class="p">,</span> <span class="n">LowerTriangularMask</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Unsupported attn_bias type&quot;</span><span class="p">)</span>
        <span class="n">causal</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">attn_bias</span><span class="p">,</span> <span class="n">LowerTriangularMask</span><span class="p">)</span>
        <span class="n">out</span><span class="p">,</span> <span class="n">lse</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">FORWARD_OPERATOR</span><span class="p">(</span>
            <span class="n">query</span><span class="o">=</span><span class="n">query</span><span class="p">,</span>
            <span class="n">key</span><span class="o">=</span><span class="n">key</span><span class="p">,</span>
            <span class="n">value</span><span class="o">=</span><span class="n">value</span><span class="p">,</span>
            <span class="n">cu_seqlens_q</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">cu_seqlens_k</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">max_seqlen_q</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">compute_logsumexp</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">causal</span><span class="o">=</span><span class="n">causal</span><span class="p">,</span>
            <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">lse</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">p</span> <span class="o">=</span> <span class="n">p</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">causal</span> <span class="o">=</span> <span class="n">causal</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">uses_tensorcores</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">d</span><span class="p">:</span> <span class="s2">&quot;AttentionOpDispatch&quot;</span><span class="p">,</span> <span class="n">is_half</span><span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="n">sm_major</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_capability</span><span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">device</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">sm_major</span> <span class="o">&gt;=</span> <span class="mi">8</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="n">sm_major</span> <span class="o">&gt;=</span> <span class="mi">7</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">is_half</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">supports</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">d</span><span class="p">:</span> <span class="s2">&quot;AttentionOpDispatch&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">super</span><span class="p">(</span><span class="n">MemoryEfficientAttentionCutlassOp</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">supports</span><span class="p">(</span><span class="n">d</span><span class="p">):</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="n">cap</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_capability</span><span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">sm</span> <span class="o">=</span> <span class="n">cap</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">10</span> <span class="o">+</span> <span class="n">cap</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">bits_per_scalar</span> <span class="o">=</span> <span class="p">{</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">half</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">:</span> <span class="mi">16</span><span class="p">}[</span><span class="n">d</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span>
        <span class="n">uses_tensorcores</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">uses_tensorcores</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">bits_per_scalar</span> <span class="o">==</span> <span class="mi">16</span><span class="p">)</span>
        <span class="n">matmul_alignment_mn</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">sm</span> <span class="o">&gt;=</span> <span class="mi">80</span><span class="p">:</span>
            <span class="n">matmul_alignment_mn</span> <span class="o">=</span> <span class="mi">4</span>
        <span class="k">if</span> <span class="n">uses_tensorcores</span><span class="p">:</span>
            <span class="n">matmul_alignment_mn</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">matmul_alignment_mn</span><span class="p">,</span> <span class="mi">128</span> <span class="o">//</span> <span class="n">bits_per_scalar</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">k</span> <span class="o">%</span> <span class="n">matmul_alignment_mn</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">kv</span> <span class="o">%</span> <span class="n">matmul_alignment_mn</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="c1"># Sm86 does not have enough shared-memory</span>
        <span class="c1"># See https://github.com/facebookresearch/xformers/issues/517</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">d</span><span class="o">.</span><span class="n">requires_grad</span>
            <span class="ow">and</span> <span class="n">sm</span> <span class="o">&gt;=</span> <span class="mi">80</span>
            <span class="ow">and</span> <span class="n">sm</span> <span class="o">!=</span> <span class="mi">80</span>
            <span class="ow">and</span> <span class="n">d</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">is</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span>
            <span class="ow">and</span> <span class="nb">max</span><span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">kv</span><span class="p">,</span> <span class="n">d</span><span class="o">.</span><span class="n">k</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">64</span>
        <span class="p">):</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">ctx</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
        <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">lse</span><span class="p">,</span> <span class="n">out</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>

        <span class="n">dtype</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">dtype</span>
        <span class="p">(</span>
            <span class="n">grad_q</span><span class="p">,</span>
            <span class="n">grad_k</span><span class="p">,</span>
            <span class="n">grad_v</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">xformers</span><span class="o">.</span><span class="n">efficient_attention_backward_cutlass</span><span class="p">(</span>
            <span class="n">grad</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">),</span>
            <span class="n">query</span><span class="p">,</span>
            <span class="n">key</span><span class="p">,</span>
            <span class="n">value</span><span class="p">,</span>
            <span class="n">lse</span><span class="p">,</span>
            <span class="n">out</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">),</span>
            <span class="n">causal</span><span class="o">=</span><span class="n">ctx</span><span class="o">.</span><span class="n">causal</span><span class="p">,</span>
            <span class="n">scale</span><span class="o">=</span><span class="n">ctx</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">grad_q</span><span class="p">,</span> <span class="n">grad_k</span><span class="p">,</span> <span class="n">grad_v</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="MemoryEfficientAttentionFlashAttentionOp"><a class="viewcode-back" href="../../../components/ops.html#xformers.ops.MemoryEfficientAttentionFlashAttentionOp">[docs]</a><span class="k">class</span> <span class="nc">MemoryEfficientAttentionFlashAttentionOp</span><span class="p">(</span><span class="n">AttentionOpBase</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Operator that computes memory-efficient attention using \</span>
<span class="sd">        `Flash-Attention &lt;https://github.com/HazyResearch/flash-attention&gt;`_ \</span>
<span class="sd">        implementation.</span>


<span class="sd">    This is a wrapper to make FlashAttention compatible with xformers&#39;s API</span>
<span class="sd">    Most of this code was taken from:</span>
<span class="sd">    https://github.com/HazyResearch/flash-attention/blob/main/flash_attn/flash_attn_interface.py</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">FORWARD_OPERATOR</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">SUPPORTED_DEVICES</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;cuda&quot;</span><span class="p">}</span>
    <span class="n">SUPPORTED_DTYPES</span> <span class="o">=</span> <span class="p">{</span><span class="n">torch</span><span class="o">.</span><span class="n">half</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">}</span>
    <span class="n">SUPPORTED_MAX_K</span> <span class="o">=</span> <span class="mi">128</span>
    <span class="n">SUPPORTED_ATTN_BIAS_TYPES</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="nb">type</span><span class="p">(</span><span class="kc">None</span><span class="p">),</span> <span class="n">LowerTriangularMask</span><span class="p">}</span>
    <span class="n">SUPPORTS_DROPOUT</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">SUPPORTS_CUSTOM_SCALE</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">SUPPORTS_DIFFERENT_VALUE_EMBED</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">NAME</span> <span class="o">=</span> <span class="s2">&quot;flshatt&quot;</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">info</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">has_flashattention</span><span class="p">:</span>
            <span class="k">return</span> <span class="s2">&quot;not built&quot;</span>
        <span class="k">return</span> <span class="s2">&quot;available - requires GPU with compute capability 7.5+&quot;</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">supports</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">d</span><span class="p">:</span> <span class="s2">&quot;AttentionOpDispatch&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">has_flashattention</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">super</span><span class="p">(</span><span class="n">MemoryEfficientAttentionFlashAttentionOp</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">supports</span><span class="p">(</span><span class="n">d</span><span class="p">):</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="c1"># We know `d.device` is cuda now</span>
        <span class="c1"># d=128 is only supported on A100 for bw</span>
        <span class="n">device_capability</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_capability</span><span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">is_sm80</span> <span class="o">=</span> <span class="n">device_capability</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">8</span> <span class="ow">and</span> <span class="n">device_capability</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="n">d</span><span class="o">.</span><span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">]:</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">d</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">and</span> <span class="n">d</span><span class="o">.</span><span class="n">k</span> <span class="o">==</span> <span class="mi">128</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_sm80</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="k">return</span> <span class="n">device_capability</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">forward_no_grad</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">query</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">key</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attn_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">AttentionMask</span><span class="p">]],</span>
        <span class="n">p</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
            <span class="n">ctx</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">query</span><span class="o">=</span><span class="n">query</span><span class="p">,</span>
            <span class="n">key</span><span class="o">=</span><span class="n">key</span><span class="p">,</span>
            <span class="n">value</span><span class="o">=</span><span class="n">value</span><span class="p">,</span>
            <span class="n">attn_bias</span><span class="o">=</span><span class="n">attn_bias</span><span class="p">,</span>
            <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span>
            <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">prepare_inputs</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span> <span class="n">ctx</span><span class="p">,</span> <span class="n">query</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
    <span class="p">):</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">seqlen_q</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">seqlen_k</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">num_heads</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">head_dim_q</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
        <span class="n">head_dim_v</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">max_seqlen_q</span> <span class="o">=</span> <span class="n">seqlen_q</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">max_seqlen_k</span> <span class="o">=</span> <span class="n">seqlen_k</span>

        <span class="n">cu_seqlens_k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span>
            <span class="mi">0</span><span class="p">,</span>
            <span class="p">(</span><span class="n">batch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">seqlen_k</span><span class="p">,</span>
            <span class="n">step</span><span class="o">=</span><span class="n">seqlen_k</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">query</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">seqlen_q</span> <span class="o">==</span> <span class="n">seqlen_k</span><span class="p">:</span>
            <span class="n">cu_seqlens_q</span> <span class="o">=</span> <span class="n">cu_seqlens_k</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">cu_seqlens_q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span>
                <span class="mi">0</span><span class="p">,</span>
                <span class="p">(</span><span class="n">batch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">seqlen_q</span><span class="p">,</span>
                <span class="n">step</span><span class="o">=</span><span class="n">seqlen_q</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="n">query</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Initially we have `query.shape = [batch, seqlen, head_dim_q]`</span>
        <span class="c1"># We want format `[batch * seqlen, num_heads, head_dim_q]`</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">query_api_input_shape</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">key_api_input_shape</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">value_api_input_shape</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="n">batch</span> <span class="o">*</span> <span class="n">seqlen_q</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">head_dim_q</span><span class="p">])</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="n">batch</span> <span class="o">*</span> <span class="n">seqlen_k</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">head_dim_q</span><span class="p">])</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="n">batch</span> <span class="o">*</span> <span class="n">seqlen_k</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">head_dim_v</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">cu_seqlens_k</span><span class="p">,</span> <span class="n">cu_seqlens_q</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">ctx</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attn_bias</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">scale</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">attn_bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">attn_bias</span><span class="p">,</span> <span class="n">LowerTriangularMask</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Unsupported attn_bias type&quot;</span><span class="p">)</span>
        <span class="n">causal</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">attn_bias</span><span class="p">,</span> <span class="n">LowerTriangularMask</span><span class="p">)</span>
        <span class="n">return_softmax</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">ctx_flash</span> <span class="o">=</span> <span class="n">ctx</span> <span class="k">if</span> <span class="n">ctx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">SimpleNamespace</span><span class="p">()</span>
        <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">cu_seqlens_k</span><span class="p">,</span> <span class="n">cu_seqlens_q</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">prepare_inputs</span><span class="p">(</span>
            <span class="n">ctx_flash</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span>
        <span class="p">)</span>

        <span class="c1"># Save rng_state because the backward pass will regenerate the dropout mask</span>
        <span class="n">rng_state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_rng_state</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">softmax_scale</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">)</span> <span class="k">if</span> <span class="n">scale</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">scale</span>
        <span class="n">out</span><span class="p">,</span> <span class="n">softmax_lse</span><span class="p">,</span> <span class="n">S_dmask</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_flash_attn_forward</span><span class="p">(</span>
            <span class="n">query</span><span class="p">,</span>
            <span class="n">key</span><span class="p">,</span>
            <span class="n">value</span><span class="p">,</span>
            <span class="n">cu_seqlens_q</span><span class="p">,</span>
            <span class="n">cu_seqlens_k</span><span class="p">,</span>
            <span class="n">ctx_flash</span><span class="o">.</span><span class="n">max_seqlen_q</span><span class="p">,</span>
            <span class="n">ctx_flash</span><span class="o">.</span><span class="n">max_seqlen_k</span><span class="p">,</span>
            <span class="n">p</span><span class="p">,</span>
            <span class="n">softmax_scale</span><span class="p">,</span>
            <span class="n">causal</span><span class="o">=</span><span class="n">causal</span><span class="p">,</span>
            <span class="n">return_softmax</span><span class="o">=</span><span class="n">return_softmax</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">ctx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span>
                <span class="n">query</span><span class="p">,</span>
                <span class="n">key</span><span class="p">,</span>
                <span class="n">value</span><span class="p">,</span>
                <span class="n">out</span><span class="p">,</span>
                <span class="n">softmax_lse</span><span class="p">,</span>
                <span class="n">cu_seqlens_q</span><span class="p">,</span>
                <span class="n">cu_seqlens_k</span><span class="p">,</span>
                <span class="n">rng_state</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">ctx</span><span class="o">.</span><span class="n">dropout_p</span> <span class="o">=</span> <span class="n">p</span>
            <span class="n">ctx</span><span class="o">.</span><span class="n">softmax_scale</span> <span class="o">=</span> <span class="n">softmax_scale</span>
            <span class="n">ctx</span><span class="o">.</span><span class="n">causal</span> <span class="o">=</span> <span class="n">causal</span>
            <span class="n">ctx</span><span class="o">.</span><span class="n">kernel_output_shape</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">ctx</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_backward</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">ctx</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">saved_tensors</span><span class="p">):</span>
        <span class="p">(</span>
            <span class="n">q</span><span class="p">,</span>
            <span class="n">k</span><span class="p">,</span>
            <span class="n">v</span><span class="p">,</span>
            <span class="n">out</span><span class="p">,</span>
            <span class="n">softmax_lse</span><span class="p">,</span>
            <span class="n">cu_seqlens_q</span><span class="p">,</span>
            <span class="n">cu_seqlens_k</span><span class="p">,</span>
            <span class="n">rng_state</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">=</span> <span class="n">saved_tensors</span>
        <span class="k">if</span> <span class="n">rng_state</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">cur_rng_state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_rng_state</span><span class="p">()</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_rng_state</span><span class="p">(</span><span class="n">rng_state</span><span class="p">)</span>
        <span class="c1"># Create dq,dk,dv</span>
        <span class="c1"># If Q/K/V come from a single QKV tensor, let&#39;s put the gradient in the</span>
        <span class="c1"># right strides, so we can avoid a `cat`</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">k</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="ow">and</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
            <span class="ow">and</span> <span class="n">q</span><span class="o">.</span><span class="n">storage</span><span class="p">()</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span> <span class="o">==</span> <span class="n">k</span><span class="o">.</span><span class="n">storage</span><span class="p">()</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span>
            <span class="ow">and</span> <span class="n">q</span><span class="o">.</span><span class="n">storage</span><span class="p">()</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span> <span class="o">==</span> <span class="n">v</span><span class="o">.</span><span class="n">storage</span><span class="p">()</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span>
        <span class="p">):</span>
            <span class="c1"># Create one big contiguous chunk</span>
            <span class="c1"># This is because q, k and v usually come from a single</span>
            <span class="c1"># output of a linear layer that is chunked.</span>
            <span class="c1"># Creating the gradients with the right layout saves us</span>
            <span class="c1"># a `torch.cat` call in the backward pass</span>
            <span class="n">chunk</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
                <span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">3</span><span class="p">,</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">q</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">q</span><span class="o">.</span><span class="n">device</span>
            <span class="p">)</span>
            <span class="n">dq</span> <span class="o">=</span> <span class="n">chunk</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
            <span class="n">dk</span> <span class="o">=</span> <span class="n">chunk</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">dv</span> <span class="o">=</span> <span class="n">chunk</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dq</span><span class="p">,</span> <span class="n">dk</span><span class="p">,</span> <span class="n">dv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">q</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">k</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>

        <span class="k">assert</span> <span class="n">grad</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">SUPPORTED_DTYPES</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">_flash_attn_backward</span><span class="p">(</span>
            <span class="n">grad</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">kernel_output_shape</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">(),</span>
            <span class="n">q</span><span class="p">,</span>
            <span class="n">k</span><span class="p">,</span>
            <span class="n">v</span><span class="p">,</span>
            <span class="n">out</span><span class="p">,</span>
            <span class="n">softmax_lse</span><span class="p">,</span>
            <span class="n">dq</span><span class="p">,</span>
            <span class="n">dk</span><span class="p">,</span>
            <span class="n">dv</span><span class="p">,</span>
            <span class="n">cu_seqlens_q</span><span class="p">,</span>
            <span class="n">cu_seqlens_k</span><span class="p">,</span>
            <span class="n">ctx</span><span class="o">.</span><span class="n">max_seqlen_q</span><span class="p">,</span>
            <span class="n">ctx</span><span class="o">.</span><span class="n">max_seqlen_k</span><span class="p">,</span>
            <span class="n">ctx</span><span class="o">.</span><span class="n">dropout_p</span><span class="p">,</span>
            <span class="n">ctx</span><span class="o">.</span><span class="n">softmax_scale</span><span class="p">,</span>
            <span class="n">ctx</span><span class="o">.</span><span class="n">causal</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">rng_state</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_rng_state</span><span class="p">(</span><span class="n">cur_rng_state</span><span class="p">)</span>
        <span class="n">dq</span> <span class="o">=</span> <span class="n">dq</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">query_api_input_shape</span><span class="p">)</span>
        <span class="n">dk</span> <span class="o">=</span> <span class="n">dk</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">key_api_input_shape</span><span class="p">)</span>
        <span class="n">dv</span> <span class="o">=</span> <span class="n">dv</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">value_api_input_shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">dq</span><span class="p">,</span> <span class="n">dk</span><span class="p">,</span> <span class="n">dv</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_flash_attn_forward</span><span class="p">(</span>
        <span class="n">q</span><span class="p">,</span>
        <span class="n">k</span><span class="p">,</span>
        <span class="n">v</span><span class="p">,</span>
        <span class="n">cu_seqlens_q</span><span class="p">,</span>
        <span class="n">cu_seqlens_k</span><span class="p">,</span>
        <span class="n">max_seqlen_q</span><span class="p">,</span>
        <span class="n">max_seqlen_k</span><span class="p">,</span>
        <span class="n">dropout_p</span><span class="p">,</span>
        <span class="n">softmax_scale</span><span class="p">,</span>
        <span class="n">causal</span><span class="p">,</span>
        <span class="n">return_softmax</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">out</span><span class="p">,</span> <span class="n">softmax_lse</span><span class="p">,</span> <span class="o">*</span><span class="n">rest</span> <span class="o">=</span> <span class="n">_C_flashattention</span><span class="o">.</span><span class="n">fwd</span><span class="p">(</span>
            <span class="n">q</span><span class="p">,</span>
            <span class="n">k</span><span class="p">,</span>
            <span class="n">v</span><span class="p">,</span>
            <span class="n">cu_seqlens_q</span><span class="p">,</span>
            <span class="n">cu_seqlens_k</span><span class="p">,</span>
            <span class="n">max_seqlen_q</span><span class="p">,</span>
            <span class="n">max_seqlen_k</span><span class="p">,</span>
            <span class="n">dropout_p</span><span class="p">,</span>
            <span class="n">softmax_scale</span><span class="p">,</span>
            <span class="kc">False</span><span class="p">,</span>
            <span class="n">causal</span><span class="p">,</span>
            <span class="n">return_softmax</span><span class="p">,</span>
            <span class="kc">None</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">S_dmask</span> <span class="o">=</span> <span class="n">rest</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">return_softmax</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">softmax_lse</span><span class="p">,</span> <span class="n">S_dmask</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_flash_attn_backward</span><span class="p">(</span>
        <span class="n">dout</span><span class="p">,</span>
        <span class="n">q</span><span class="p">,</span>
        <span class="n">k</span><span class="p">,</span>
        <span class="n">v</span><span class="p">,</span>
        <span class="n">out</span><span class="p">,</span>
        <span class="n">softmax_lse</span><span class="p">,</span>
        <span class="n">dq</span><span class="p">,</span>
        <span class="n">dk</span><span class="p">,</span>
        <span class="n">dv</span><span class="p">,</span>
        <span class="n">cu_seqlens_q</span><span class="p">,</span>
        <span class="n">cu_seqlens_k</span><span class="p">,</span>
        <span class="n">max_seqlen_q</span><span class="p">,</span>
        <span class="n">max_seqlen_k</span><span class="p">,</span>
        <span class="n">dropout_p</span><span class="p">,</span>
        <span class="n">softmax_scale</span><span class="p">,</span>
        <span class="n">causal</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">softmax_d</span> <span class="o">=</span> <span class="n">_C_flashattention</span><span class="o">.</span><span class="n">bwd</span><span class="p">(</span>
            <span class="n">dout</span><span class="p">,</span>
            <span class="n">q</span><span class="p">,</span>
            <span class="n">k</span><span class="p">,</span>
            <span class="n">v</span><span class="p">,</span>
            <span class="n">out</span><span class="p">,</span>
            <span class="n">softmax_lse</span><span class="p">,</span>
            <span class="n">dq</span><span class="p">,</span>
            <span class="n">dk</span><span class="p">,</span>
            <span class="n">dv</span><span class="p">,</span>
            <span class="n">cu_seqlens_q</span><span class="p">,</span>
            <span class="n">cu_seqlens_k</span><span class="p">,</span>
            <span class="n">max_seqlen_q</span><span class="p">,</span>
            <span class="n">max_seqlen_k</span><span class="p">,</span>
            <span class="n">dropout_p</span><span class="p">,</span>
            <span class="n">softmax_scale</span><span class="p">,</span>
            <span class="kc">False</span><span class="p">,</span>
            <span class="n">causal</span><span class="p">,</span>
            <span class="kc">None</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">dq</span><span class="p">,</span> <span class="n">dk</span><span class="p">,</span> <span class="n">dv</span><span class="p">,</span> <span class="n">softmax_d</span></div>


<div class="viewcode-block" id="MemoryEfficientAttentionCutlassFwdFlashBwOp"><a class="viewcode-back" href="../../../components/ops.html#xformers.ops.MemoryEfficientAttentionCutlassFwdFlashBwOp">[docs]</a><span class="k">class</span> <span class="nc">MemoryEfficientAttentionCutlassFwdFlashBwOp</span><span class="p">(</span><span class="n">MemoryEfficientAttentionCutlassOp</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;An operator that uses :attr:`xformers.ops.MemoryEfficientAttentionCutlassOp` for the forward pass \</span>
<span class="sd">        and :attr:`xformers.ops.MemoryEfficientAttentionFlashAttentionOp` for the backward.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">FW_OP</span> <span class="o">=</span> <span class="n">MemoryEfficientAttentionCutlassOp</span>
    <span class="n">BW_OP</span> <span class="o">=</span> <span class="n">MemoryEfficientAttentionFlashAttentionOp</span>
    <span class="n">SUPPORTS_CUSTOM_SCALE</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">SUPPORTED_DTYPES</span> <span class="o">=</span> <span class="n">BW_OP</span><span class="o">.</span><span class="n">SUPPORTED_DTYPES</span><span class="o">.</span><span class="n">intersection</span><span class="p">(</span><span class="n">FW_OP</span><span class="o">.</span><span class="n">SUPPORTED_DTYPES</span><span class="p">)</span>

    <span class="n">NAME</span> <span class="o">=</span> <span class="s2">&quot;fctls_bflsh&quot;</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">supports</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">d</span><span class="p">:</span> <span class="s2">&quot;AttentionOpDispatch&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">d</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">cls</span><span class="o">.</span><span class="n">BW_OP</span><span class="o">.</span><span class="n">supports</span><span class="p">(</span><span class="n">d</span><span class="p">):</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">FW_OP</span><span class="o">.</span><span class="n">supports</span><span class="p">(</span><span class="n">replace</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">ctx</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
        <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">lse</span><span class="p">,</span> <span class="n">out</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
        <span class="n">ctx_flash</span> <span class="o">=</span> <span class="n">SimpleNamespace</span><span class="p">()</span>

        <span class="n">ctx_flash</span><span class="o">.</span><span class="n">causal</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">causal</span>
        <span class="n">ctx_flash</span><span class="o">.</span><span class="n">dropout_p</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">cu_seqlens_k</span><span class="p">,</span> <span class="n">cu_seqlens_q</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">BW_OP</span><span class="o">.</span><span class="n">prepare_inputs</span><span class="p">(</span>
            <span class="n">ctx_flash</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span>
        <span class="p">)</span>
        <span class="n">ctx_flash</span><span class="o">.</span><span class="n">kernel_output_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
        <span class="n">ctx_flash</span><span class="o">.</span><span class="n">softmax_scale</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">)</span> <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">scale</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">ctx</span><span class="o">.</span><span class="n">scale</span>
        <span class="p">)</span>
        <span class="n">rng_state</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">ctx_flash</span><span class="o">.</span><span class="n">kernel_output_shape</span><span class="p">)</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">ctx_flash</span><span class="o">.</span><span class="n">kernel_output_shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">BW_OP</span><span class="o">.</span><span class="n">_backward</span><span class="p">(</span>
            <span class="n">ctx_flash</span><span class="p">,</span>
            <span class="n">grad</span><span class="p">,</span>
            <span class="p">[</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">lse</span><span class="p">,</span> <span class="n">cu_seqlens_q</span><span class="p">,</span> <span class="n">cu_seqlens_k</span><span class="p">,</span> <span class="n">rng_state</span><span class="p">],</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="AttentionOpDispatch"><a class="viewcode-back" href="../../../components/ops.html#xformers.ops.AttentionOpDispatch">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">AttentionOpDispatch</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Dispatcher to automatically select</span>
<span class="sd">    the best operator to run memory-efficient attention.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span>
    <span class="n">k</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">has_dropout</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="n">attn_bias_type</span><span class="p">:</span> <span class="n">Any</span>
    <span class="n">kv_len</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">q_len</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">kv</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">has_custom_scale</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span>

    <span class="k">def</span> <span class="nf">_is_cutlass_fwd_faster_than_flash</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="c1"># Very small batch sizes - if batch size specified</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">threads_flash</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span>
            <span class="n">threads_cutlass</span> <span class="o">=</span> <span class="n">threads_flash</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_len</span> <span class="o">//</span> <span class="mi">64</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">threads_flash</span> <span class="o">&lt;</span> <span class="mi">60</span> <span class="ow">and</span> <span class="p">(</span><span class="n">threads_cutlass</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">threads_flash</span><span class="p">:</span>
                <span class="k">return</span> <span class="kc">True</span>
        <span class="c1"># Large values of K</span>
        <span class="k">return</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv</span><span class="p">)</span> <span class="o">==</span> <span class="mi">128</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">op</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AttentionOp</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Computes the best operator</span>

<span class="sd">        Raises:</span>
<span class="sd">            NotImplementedError: if not operator was found</span>

<span class="sd">        Returns:</span>
<span class="sd">            AttentionOp: The best operator for the configuration</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">priority_list_ops</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">AttentionOp</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">MemoryEfficientAttentionFlashAttentionOp</span><span class="p">,</span>
            <span class="n">MemoryEfficientAttentionCutlassOp</span><span class="p">,</span>
            <span class="n">MemoryEfficientAttentionOp</span><span class="p">,</span>
        <span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_cutlass_fwd_faster_than_flash</span><span class="p">():</span>
            <span class="n">priority_list_ops</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">MemoryEfficientAttentionCutlassFwdFlashBwOp</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">op</span> <span class="ow">in</span> <span class="n">priority_list_ops</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">op</span><span class="o">.</span><span class="n">supports</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">op</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;No operator found for this attention: </span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<div class="viewcode-block" id="AttentionOpDispatch.from_arguments"><a class="viewcode-back" href="../../../components/ops.html#xformers.ops.AttentionOpDispatch.from_arguments">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_arguments</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">query</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">key</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attn_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">AttentionMask</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;AttentionOpDispatch&quot;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Creates an :attr:`xformers.ops.AttentionOpDispatch` from :attr:`xformers.ops.memory_efficient_attention`&#39;s</span>
<span class="sd">        arguments</span>

<span class="sd">        Args:</span>
<span class="sd">            query (torch.Tensor)</span>
<span class="sd">            key (torch.Tensor)</span>
<span class="sd">            value (torch.Tensor)</span>
<span class="sd">            attn_bias (Optional[Union[torch.Tensor, xformers.ops.AttentionMask]], optional): Defaults to None.</span>
<span class="sd">            p (float, optional): Defaults to 0.0.</span>
<span class="sd">            scale (float, optional): Custom scale. Default to None (use q.shape[-1]**-0.5).</span>

<span class="sd">        Returns:</span>
<span class="sd">            AttentionOpDispatch</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">H</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">query</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">H</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">AttentionOpDispatch</span><span class="p">(</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">query</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="n">k</span><span class="o">=</span><span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">kv</span><span class="o">=</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">has_dropout</span><span class="o">=</span><span class="n">p</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">,</span>
            <span class="n">has_custom_scale</span><span class="o">=</span><span class="n">scale</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">attn_bias_type</span><span class="o">=</span><span class="nb">type</span><span class="p">(</span><span class="n">attn_bias</span><span class="p">),</span>
            <span class="n">kv_len</span><span class="o">=</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">q_len</span><span class="o">=</span><span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">B</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="o">=</span><span class="n">H</span><span class="p">,</span>
            <span class="n">requires_grad</span><span class="o">=</span><span class="nb">any</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">]),</span>
        <span class="p">)</span></div></div>


<div class="viewcode-block" id="memory_efficient_attention"><a class="viewcode-back" href="../../../components/ops.html#xformers.ops.memory_efficient_attention">[docs]</a><span class="k">def</span> <span class="nf">memory_efficient_attention</span><span class="p">(</span>
    <span class="n">query</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">key</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">attn_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">AttentionMask</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">op</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AttentionOp</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Implements the memory-efficient attention mechanism following</span>
<span class="sd">    `&quot;Self-Attention Does Not Need O(n^2) Memory&quot; &lt;http://arxiv.org/abs/2112.05682&gt;`_.</span>

<span class="sd">    :Inputs shape:</span>

<span class="sd">    - Input tensors must be in format ``[B, M, H, K]``, where B is the batch size, M \</span>
<span class="sd">        the sequence length, H the number of heads, and K the embeding size per head</span>

<span class="sd">    - If inputs have dimension 3, it is assumed that the dimensions are ``[B, M, K]`` and ``H=1``</span>

<span class="sd">    - Inputs can be non-contiguous - we only require the last dimension&#39;s stride to be 1</span>


<span class="sd">    :Equivalent pytorch code:</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        scale = 1 / query.shape[-1] ** 0.5</span>
<span class="sd">        query = query * scale</span>
<span class="sd">        attn = query @ key.transpose(-2, -1)</span>
<span class="sd">        if attn_bias is not None:</span>
<span class="sd">            attn = attn + attn_bias</span>
<span class="sd">        attn = attn.softmax(-1)</span>
<span class="sd">        attn = F.dropout(attn, p)</span>
<span class="sd">        return attn @ value</span>

<span class="sd">    :Examples:</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        import xformers.ops as xops</span>

<span class="sd">        # Compute regular attention</span>
<span class="sd">        y = xops.memory_efficient_attention(q, k, v)</span>

<span class="sd">        # With a dropout of 0.2</span>
<span class="sd">        y = xops.memory_efficient_attention(q, k, v, p=0.2)</span>

<span class="sd">        # Causal attention</span>
<span class="sd">        y = xops.memory_efficient_attention(</span>
<span class="sd">            q, k, v,</span>
<span class="sd">            attn_bias=xops.LowerTriangularMask()</span>
<span class="sd">        )</span>

<span class="sd">    :Supported hardware:</span>

<span class="sd">        NVIDIA GPUs with compute capability above 6.0 (P100+), datatype ``f16``, ``bf16`` and ``f32``.</span>

<span class="sd">    Raises:</span>
<span class="sd">        NotImplementedError: if there is no operator available to compute the MHA</span>

<span class="sd">    :parameter query: Tensor of shape ``[B, Mq, H, K]``</span>
<span class="sd">    :parameter key: Tensor of shape ``[B, Mkv, H, K]``</span>
<span class="sd">    :parameter value: Tensor of shape ``[B, Mkv, H, Kv]``</span>
<span class="sd">    :parameter attn_bias: Bias to apply to the attention matrix - defaults to no masking. \</span>
<span class="sd">        For causal attention, use :attr:`xformers.ops.LowerTriangularMask`. \</span>
<span class="sd">        This can also be a :attr:`torch.Tensor` for an arbitrary mask.</span>
<span class="sd">    :parameter p: Dropout probability. Disabled if set to ``0.0``</span>
<span class="sd">    :parameter scale: The scale to query_state weights. If set to ``None``, the default \</span>
<span class="sd">        scale (q.shape[-1]**-0.5) will be used.</span>
<span class="sd">    :parameter op: The operator to use - see :attr:`xformers.ops.AttentionOpBase`. \</span>
<span class="sd">        If set to ``None`` (recommended), xFormers \</span>
<span class="sd">        will dispatch to the best available operator, depending on the inputs \</span>
<span class="sd">        and options.</span>
<span class="sd">    :return: multi-head attention Tensor with shape ``[B, Mq, H, Kv]``</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">query</span><span class="o">.</span><span class="n">ndim</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Invalid shape for query: </span><span class="si">{</span><span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">. &quot;</span>
            <span class="s2">&quot;Expected shape [batch, seqlen, num_heads, K], or [batch, seqlen, K].&quot;</span>
        <span class="p">)</span>
    <span class="n">output_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">+</span> <span class="p">(</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],)</span>
    <span class="c1"># Convert from legacy format</span>
    <span class="k">if</span> <span class="n">query</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">op</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">op</span> <span class="o">=</span> <span class="n">AttentionOpDispatch</span><span class="o">.</span><span class="n">from_arguments</span><span class="p">(</span>
            <span class="n">query</span><span class="o">=</span><span class="n">query</span><span class="p">,</span>
            <span class="n">key</span><span class="o">=</span><span class="n">key</span><span class="p">,</span>
            <span class="n">value</span><span class="o">=</span><span class="n">value</span><span class="p">,</span>
            <span class="n">attn_bias</span><span class="o">=</span><span class="n">attn_bias</span><span class="p">,</span>
            <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span>
            <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">op</span>

    <span class="c1"># fast-path that doesn&#39;t require computing the logsumexp for backward computation</span>
    <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">is</span> <span class="kc">False</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">]):</span>
        <span class="k">return</span> <span class="n">op</span><span class="o">.</span><span class="n">forward_no_grad</span><span class="p">(</span>
            <span class="n">query</span><span class="o">=</span><span class="n">query</span><span class="p">,</span>
            <span class="n">key</span><span class="o">=</span><span class="n">key</span><span class="p">,</span>
            <span class="n">value</span><span class="o">=</span><span class="n">value</span><span class="p">,</span>
            <span class="n">attn_bias</span><span class="o">=</span><span class="n">attn_bias</span><span class="p">,</span>
            <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span>
            <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">output_shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">op</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attn_bias</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">output_shape</span><span class="p">)</span></div>
</pre></div>

              </article>
              
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright Copyright © 2021 Meta Platforms, Inc.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <!-- <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div> -->
    </section>
  </div>

  


  

  
  <script type="text/javascript" id="documentation_options" data-url_root="../../../"
    src="../../../_static/documentation_options.js"></script>
  <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
  <script src="../../../_static/jquery.js"></script>
  <script src="../../../_static/underscore.js"></script>
  <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
  <script src="../../../_static/doctools.js"></script>
  

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->


  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://github.com/facebookresearch/xformers" class="footer-logo"></a>
      </div>
      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://github.com/facebookresearch/xformers">fairscale</a></li>
            <li><a href="https://github.com/facebookresearch/xformers/blob/master/README.md">Get Started</a></li>
            <li><a href="https://github.com/facebookresearch/xformers/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="">Resources</a></li>
            <li><a href="https://github.com/facebookresearch/xformers">Docs</a></li>
            <li><a href="https://github.com/facebookresearch/xformers/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://opensource.facebook.com/legal/terms">Terms of Use</a></li>
            <li><a href="https://opensource.facebook.com/legal/privacy">Privacy Policy</a></li>
          </ul>
        </div>
      </div>
    </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://github.com/facebookresearch/xformers" aria-label="fairscale"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/facebookresearch/xformers/blob/master/README.md">Get Started</a>
          </li>

          <li>
            <a href="https://github.com/facebookresearch/xformers">Docs</a>
          </li>

          <li>
            <a href="https://github.com/facebookresearch/xformers">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function () {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function (e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>

</html>